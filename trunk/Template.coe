DATA = mygene

# ----------------------------------------------------------------------------------------
#                                     Data loading
# ----------------------------------------------------------------------------------------

# The alphabet to use:
# DNA, RNA or Protein
alphabet = DNA

# The sequence file to use (sequences must be aligned!)
input.sequence.file = $(DATA).mase
# The alignment format:
# Must be one of Mase, Fasta, Phylip, Clustal 
input.sequence.format = Mase

#Sites to use:
# all, nogap or complete (=only resolved chars)
input.sequence.sites_to_use = nogap
# Specify a maximum amount of gaps: may be an absolute number or a percentage.
input.sequence.max_gap_allowed=100%

# Second file:
input.sequence.file2 = none
# All previous options can be set up for second file, just append '2' at option names.
# The default is to use options of file1 for file2.

# The tree file to use. Branch lengths are optional.
# Only newick format is supported.
input.tree.file = $(DATA).dnd
input.tree.format = Newick

# ----------------------------------------------------------------------------------------
#                                     Model specification
# ----------------------------------------------------------------------------------------
# Read the Bio++ Program Suite manual for a detail description of all available models.
# Non-homogeneous models can be used here, and also covarions. The latter however will probably
# do not do what you want, as it will map both states and rates... use with care then.

model = HKY85(kappa=2.843, theta=0.5, useObservedFreqs=yes)

rate_distribution = Gamma(n=4, alpha=0.358)

# ----------------------------------------------------------------------------------------
#                                     Parameter estimation
# ----------------------------------------------------------------------------------------

# Should we reestimate likelihood parameters? Tree topology will not be optimized.
# (recommanded)
optimization = yes

# Method to use for optimizing numerical parameters:
# -DB (default) derivatives for branch lengths + Brent for other parameters
# -fullD derivatives for all parameters, using numerical derivatives for non-branch lengths parameters.
optimization.method = DB

# Number of progressive steps to use in DB optimization (a value of 5 is often fine).
# 1 means no progressive optimization.
optimization.method_DB.nstep = 1

# Derivatives-based algorithm to us:
# - newton (recommanded) uses first and second order derivatives
# - gradient uses only first-order derivatives
optimization.method.derivatives = newton

# Final optimization step, may be useful if numerical derivatives are used:
# powell or simplex, leave the field empty for no final optimization step.
optimization.final = 

# Parameters to ignore (for instance equilibrium frequencies)
optimization.ignore_parameter =

# A file where to dump optimization steps (a file path or std for standard output)
optimization.profiler = $(DATA).profile

# idem for error or warning messages:
optimization.message_handler = $(DATA).messages

# Maximum number of likelihood evaluations:
optimization.max_number_f_eval = 1000000

# Precision to reach:
optimization.tolerance = 0.000001

# Set the quantity of output to the screen:
optimization.verbose = 1

# Should we write the optimized tree? none or file name.
output.tree.file = $(DATA).opt.dnd

# A file where site-specific information will be written (posterior rate, constant site, etc.):
output.infos = $(DATA).infos

# ----------------------------------------------------------------------------------------
#                                     Substitution vectors
# ----------------------------------------------------------------------------------------

# We may restart an analysis by specifying the already computed vectors:
input.vectors.file = none
# Otherwise, tell where to write vectors:
output.vectors.file = $(DATA).vec

# The method to use to compute vectors:
# One of
# - laplace: real mapping
# - simple: naive mapping (1 substitution if x<>y, 0 if x==y)
# - aadist: weighted mapping (proteins)
# - prob_one_jump: compute the porbability of observing at least one jump.
nijt = laplace
# Where to trunc the series (see article):
nijt_laplace.trunc = 10
# grantham, grantham.volume, grantham.polarity, charge, klein.charge, or user1 (AAIndex1 file), user2 (AAIndex2 file)
nijt_aadist.type=grantham
# Use symmetric weights? If 'yes', unsigned vectors will be computed.
nijt_aadist.sym=yes
# File paths for AAIndex files:
nijt_aadist.type_user1.file=
nijt_aadist.type_user2.file=
# Should we average over all ancestral state (yes) or use ancestral states reconstruction (no) ?
nijt.average = yes
# Should we use pair-joint calculation (yes) or marginal (no) one?
nijt.joint = yes
#!!! The 2 previous options are mainly for method comparisons, see article.

# ----------------------------------------------------------------------------------------
#                                     Additionnal stuff
# ----------------------------------------------------------------------------------------

# A file where site-specific information will be written (posterior rate, constant site, etc.):
output.infos = $(DATA).infos
# Should we write a tree with inner node names? none or file name.
output.tags.file = $(DATA).tags.dnd
# Write a file whith the correspondance between leaf names and ids.
output.tags.translation = $(DATA).tags.tln

# ----------------------------------------------------------------------------------------
#                                     Coevolution analysis
# ----------------------------------------------------------------------------------------

# 'pairwise', 'clustering' or 'candidates'
# The pairwise method corresponds to the 2005 paper
# The clustering method to the 2007 paper
# The candidates method is not published yet and is still under development, use with care!
analysis = clustering

# Coevolution statistic to use, for the pairwise and candidates methods:
# - Correlation: Dutheil et al 2005/2007's method.
# - Cosubstitution: Tuffery and Darlu 2000's method
# - Compensation: Dutheil & Galtier 2007's method.
# - MI(threshold=0.99) Mutual Information after discretization given a certain threshold (only with simple, laplace and prob_one_jump).
statistic = Correlation

# ----------------------------------------------------------------------------------------
#                                     Pairwise analysis
# ----------------------------------------------------------------------------------------

# where to write the results:
statistic.output.file = $(DATA).sta

# Minimum statistic to write in file:
statistic.min = 0

# Only write pairs with minimum rate:
statistic.min_rate = 0
# Only write pairs with minimum rate class (first class is 0):
statistic.min_rate_class = 0
# Only write pairs with site rates that differ less than... (-1 -> write all pairs):
statistic.max_rate_diff = -1.
# Only write pairs with site rate classes that differ less than... (-1 -> write all pairs):
statistic.max_rate_class_diff = -1

# Null distribution of the statistic:

# Compute null distribution of statistics (takes some time!)
statistic.null = true

# CPU*RAM pair of sites will be simulated.
# increasing the RAM parameter speeds the program but need more memory
# increasing the CPU parameter slower the program but need less memory
statistic.null.nb_rep_CPU = 100
statistic.null.nb_rep_RAM = 1000

# Where to write the null distribution
statistic.null.output.file = $(DATA).null

# Should we write the null distribution has an histogram?
# Otherwise prints all similated pairs.
statistic.null.cumul = no
# Lower bound for histogram
statistic.null.lower =
# Upper bound for histogram
statistic.null.upper =
# Number of intervals
statistic.null.nb_int =

# ----------------------------------------------------------------------------------------
#                                     Clustering Analysis
# ----------------------------------------------------------------------------------------

# Distance to use: cor (correlation) or euclidian or comp (compensation) or none
clustering.distance = cor
# Tell is mapping should be normalized (each row (=branch) will be divided by its sum).
clustering.scale = no
# Clustering algorithm: 'complete'.
# Options 'single' and 'average' are also provided, but have not been fully tested.
clustering.method = complete

# Where to write the distance matrix (in phylip format):
clustering.output.matrix.file = none
# Where to write the clustering tree (newick format):
clustering.output.tree.file = $(DATA)_clusters.dnd
# Where to write the clusters (CSV format):
clustering.output.groups.file = $(DATA)_groups.csv

# Tell if the null distribution of clusters must be computed:
clustering.null = yes
# Number of data sets to simulate:
clustering.null.number = 1000
# Where to write the simulated clusters (CSV format):
clustering.null.output.file = $(DATA)_simulation.csv

# ----------------------------------------------------------------------------------------
#                                     Selected Groups Analysis
# ----------------------------------------------------------------------------------------

candidates.input.file = $(DATA)interactions.csv
candidates.input.column_name = Group
candidates.input.column_sep = ,
 
candidates.omega = 0.2
candidates.null.min = 1000
candidates.null.nb_rep_RAM = 5000
candidates.null.verbose = 3

candidates.output.file = $(DATA)interactions_pvalues.csv
candidates.output.column_sep= , 

